####################################
# KillrWeather Reference Config File #
####################################
include "application"

spark {
  # The Spark master host. Set first by environment if exists. Then system, then config.
  # Options: spark://host1:port1,host2:port2
  # - "local" to run locally with one thread,
  # - local[4]" to run locally with 4 cores
  # - the master IP/hostname to run on a Spark standalone cluster
  # - if not set, defaults to "local[*]" to run with enough threads
  # Supports optional HA failover for more than one: host1,host2..
  # which is used to inform: spark://host1:port1,host2:port2
  master = ${?SPARK_HA_MASTER}
  cleaner.ttl = ${?SPARK_CLEANER_TTL}

  # The batch interval must be set based on the latency requirements
  # of your application and available cluster resources.
  streaming.batch.interval = ${?SPARK_STREAMING_BATCH_INTERVAL}
}

# Undefined values are expected to be set by the user in their application.conf file
# Defined values are defaults which can be overridden by the user in their application.conf file
# We first check for a setting in the environment deployed to, then whether a java system property
# exists to use, and finally the configured one. The users configured values would override
# any configured setting in this file.
# For environment keys, set the key name noted, for instance: ${?CASSANDRA_KEYSPACE}, your
# environment should be CASSANDRA_KEYSPACE="my_keyspace" or CASSANDRA_RPC_PORT=9161
cassandra {

  # The contact point to connect to the Cassandra cluster.
  # Accepts a comma-separated string of hosts. Override with -Dcassandra.connection.host.
  connection.host = ${?CASSANDRA_SEEDS}

  # Cassandra thrift port. Defaults to 9160. Override with -Dcassandra.connection.rpc.port.
  connection.rpc.port = ${?CASSANDRA_RPC_PORT}

  # Cassandra native port. Defaults to 9042. Override with -Dcassandra.connection.native.port.
  connection.native.port = ${?CASSANDRA_NATIVE_PORT}

  # Auth: These are expected to be set in the env by chef, etc.
  # The username for authentication. Override with -Dcassandra.auth.username.
  auth.username = ${?CASSANDRA_AUTH_USERNAME}
  # The password for authentication. Override with -Dcassandra.auth.password.
  auth.password = ${?CASSANDRA_AUTH_PASSWORD}

  ## Tuning ##

  # The number of milliseconds to keep unused `Cluster` object before destroying it
  # The duration to keep unused connections open. In millis, defaults to 250.
  # Override with -Dcassandra.connection.keep_alive_ms.
  connection.keep-alive = ${?CASSANDRA_KEEP_ALIVE_MS}

  # The number of times to retry a failed query. Defaults to 10.
  # Override with -Dcassandra.query.retry.count.
  connection.query.retry.count = ${?CASSANDRA_QUEUE_RETRY_COUNT}

  # The initial delay determining how often to try to reconnect to a dead node. In millis, defaults to 1000.
  # Override with -Dcassandra.connection.reconnection_delay_ms.min.
  connection.reconnect-delay.min = ${?CASSANDRA_MIN_RECONNECT_DELAY_MS}

  # The final delay determining how often to try to reconnect to a dead node. In millis, defaults to 60000.
  # Override with -Dcassandra.connection.reconnection_delay_ms.max.
  connection.reconnect-delay.max = ${?CASSANDRA_MAX_RECONNECT_DELAY_MS}

  ## Tuning: use to fine-tune the read process ##

  # To reduce the number of roundtrips to Cassandra, partitions are paged
  # The following properties control the number of partitions and the fetch size:
  # The number of rows fetched per roundtrip. Defaults to 1000.
  # Override with -Dcassandra.input.page.row.size
  read.page.row.size = ${?CASSANDRA_READ_PAGE_ROW_SIZE}

  # How many rows to fetch in a single task. Defaults to 100000.
  # Override with -Dcassandra.input.split.size
  read.split.size = ${?CASSANDRA_READ_SPLIT_SIZE}

  # The consistency level to use when reading. By default, reads are performed at
  # ConsistencyLevel.LOCAL_ONE in order to leverage data-locality and minimize network traffic.
  # Override with -Dcassandra.input.consistency.level.
  read.consistency.level = ${?CASSANDRA_READ_CONSISTENCY_LEVEL}


  ## Tuning: use to fine-tune the saving process ##

  # The maximum number of batches executed in parallel by a single task.
  # Defaults to 5. Override with -Dcassandra.output.concurrent.writes.
  write.concurrent.writes = ${?CASSANDRA_WRITE_CONCURRENT_WRITES}

  # The maximum total size of the batch in bytes; defaults to 64 kB.
  # Override with -Dcassandra.output.batch.size.bytes.
  write.batch.size.bytes = ${?CASSANDRA_WRITE_BATCH_SIZE_BYTES}

  # The number of rows per single batch; default is 'auto' which means the driver
  # will adjust the number of rows based on the amount of data in each row.
  # Override with -Dcassandra.output.batch.size.rows.
  write.batch.size.rows = ${?CASSANDRA_WRITE_BATCH_SIZE_ROWS}

  # The maximum total size of the batch in bytes. Defaults to 64 kB.
  # Override with -D
  write.max-bytes = ${?CASSANDRA_WRITE_MAX_BYTES}

  # By default, writes are performed at ConsistencyLevel.ONE in order to leverage data-locality
  # and minimize network traffic. Override with -Dcassandra.output.consistency.level
  write.consistency.level = ${?CASSANDRA_READ_CONSISTENCY_LEVEL}
}

akka {
  loglevel = "DEBUG"
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  logger-startup-timeout = 60s
  log-dead-letters = off
  log-dead-letters-during-shutdown = off

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      port = 2550
      hostname = "127.0.0.1"
    }
  }

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"

    default-dispatcher {
      # Throughput for default Dispatcher, set to 1 for as fair as possible
      throughput = 10
    }
    #serializers.kryo = "com.twitter.chill.akka.AkkaSerializer"
    #serialization-bindings."scala.Product" = kryo
  }

  cluster {
    log-info = on
    seed-nodes = []
    gossip-interval = 5s
    publish-stats-interval = 10s
    auto-down-unreachable-after = 10s
    metrics.gossip-interval = 10s
    metrics.collect-interval = 10s
  }
}

kafka {
  hosts = [${?KAFKA_HOSTS}]
  ingest-rate = 1s

  zookeeper {
    connection = ""
    port = 2181
  }
  group.id = "killrweather.group"
  topic.raw = "killrweather.raw"
  partitioner.fqcn = "kafka.producer.DefaultPartitioner"
  encoder.fqcn = "kafka.serializer.StringEncoder"
  decoder.fqcn = "kafka.serializer.StringDecoder"
  batch.send.size = 100
}
